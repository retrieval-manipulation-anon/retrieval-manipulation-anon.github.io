<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation">
  <meta name="keywords" content="Hierarchical Retrieval, Affordance Transfer, Zero-Shot Robotic Manipulation, Visual Foundation Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RAM</title>
  <style>
    figcaption {
      text-align: center;
      margin-top: 1px;
      font-size: 1rem;
      color: #111;
    }
  </style>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link rel="icon" href="./favicon.ico?">
</head>

<section class="hero">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><font color="gray">RAM</font>: <font color="gray">R</font>etrieval-Based <font color="gray">A</font>ffordance Transfer for Generalizable Zero-Shot Robotic <font color="gray">M</font>anipulation</h1>
          <div class="is-size-5 publication-authors">
            Anonymous Submission
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <img src="assets/img/teaser.png" width="75%" class="teaser-image" />
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
          Overview of our proposed RAM: (a) We extract unified affordance representation from in-the-wild multi-source demonstrations, including robotic data, HOI data, and custom data, to construct a large-scale affordance memory. Given language instructions, RAM hierarchically retrieves and transfers the 2D affordance from memory and lifts it to 3D for robotic manipulation. (b-d) Our framework shows robust generalizability across diverse objects and embodiments in various settings.
        </h2>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This work proposes a retrieve-and-transfer framework for zero-shot robotic manipulation, dubbed RAM, featuring generalizability across various objects, environments, and embodiments. Unlike existing approaches that learn manipulation from expensive in-domain demonstrations, RAM capitalizes on a retrieval-based affordance transfer paradigm to acquire versatile manipulation capabilities from abundant out-of-domain data. RAM first extracts unified affordance at scale from diverse sources of demonstrations including robotic data, human-object interaction (HOI) data, and custom data to construct a comprehensive affordance memory. Then given a language instruction, RAM hierarchically retrieves the most similar demonstration from the affordance memory and transfers such out-of-domain 2D affordance to in-domain 3D actionable affordance in a zero-shot and embodiment-agnostic manner. Extensive simulation and real-world evaluations demonstrate that our RAM consistently outperforms existing works in diverse daily tasks. Additionally, RAM shows significant potential for downstream applications such as automatic and efficient data collection, one-shot visual imitation, and LLM/VLM-integrated long-horizon manipulation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>





<section class="section">
  <div class="container is-max-widescreen content">
    <h2 class="title is-3">Real-World Rollouts (2x)</h2>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;"> </h2> -->
        <tr>
          <td>
              <h3 class="subtitle">
                All rollouts are fully autonomous, <font color="red"><b>WITHOUT</b></font> any heuristic grasping.
              </h3>
          </td>
        </tr>
        <tr>
          <td>
            <div class="content has-text-justified">
              Our method is able to perform real-world everyday tasks in a <b>zero-shot</b> manner, featuring generalizability across various objects, environments, and embodiments.
            </div>
          </td>
        </tr>
        <tr>
          <td>
            <div class="row">
              <div class="col">
                <figure>
                <video autoplay muted loop playsinline controls src="assets/video/drawer_1.mp4" width="100%"
                      style="border-radius:10px; "></video>
                <figcaption>Open the drawer</figcaption>
                </figure>
              </div>
              <div class="col">
                <figure>
                <video autoplay muted loop playsinline controls src="assets/video/microwave.mp4" width="100%"
                      style="border-radius:10px; "></video>
                <figcaption>Open the microwave</figcaption>
                </figure>
              </div>
            </div>
            <br>
            <div class="row">
              <div class="col">
                <figure>
                  <video autoplay muted loop playsinline controls src="assets/video/drawer_2.mp4" width="100%"
                         style="border-radius:10px; "></video>
                  <figcaption>Open the drawer</figcaption>
                </figure>
              </div>
              <div class="col">
                <figure>
                <video autoplay muted loop playsinline controls src="assets/video/cabinet.mp4" width="100%"
                       style="border-radius:10px; "></video>
                <figcaption>Open the cabinet</figcaption>
              </figure>
              </div>
            </div>
            <br>
            <div class="row">
              <div class="col">
                <figure>
                  <video autoplay muted loop playsinline controls src="assets/video/banana.mp4" width="100%"
                         style="border-radius:10px; "></video>
                  <figcaption>Pick up the banana</figcaption>
                </figure>
              </div>
              <div class="col">
                <figure>
                <video autoplay muted loop playsinline controls src="assets/video/bottle.mp4" width="100%"
                       style="border-radius:10px; "></video>
                <figcaption>Pick up the bottle</figcaption>
              </figure>
              </div>
            </div>
            <br>
            <div class="row">
              <div class="col">
                <figure>
                  <video autoplay muted loop playsinline controls src="assets/video/mug.mp4" width="100%"
                         style="border-radius:10px; "></video>
                  <figcaption>Pick up the mug</figcaption>
                </figure>
              </div>
              <div class="col">
                <figure>
                <video autoplay muted loop playsinline controls src="assets/video/bowl.mp4" width="100%"
                       style="border-radius:10px; "></video>
                <figcaption>Pick up the bowl</figcaption>
              </figure>
              </div>
            </div>
            <br>
          </td>
        </tr>
      </div> 
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen content">
    <h2 class="title is-3">One-Shot Visual Imitation with Human Preference (2x)</h2>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;"> </h2> -->
        <tr>
          <td>
            <div class="content has-text-justified">
              Apart from utilizing out-of-domain demonstration retrieval for manipulation, our method is naturally adaptable for one-shot visual imitation for better controllability, given a specific in-domain or out-of-domain demonstration.
            </div>
          </td>
        </tr>
        <tr>
          <td>
            <div class="row">
              <div class="col">
                <figure>
                <video autoplay muted loop playsinline controls src="assets/video/tissue_paper_human.mp4" width="100%"
                      style="border-radius:10px; "></video>
                <figcaption>Human picking up tissue <font color="red"><b>paper</b></font></figcaption>
                </figure>
              </div>
              <div class="col">
                <figure>
                <video autoplay muted loop playsinline controls src="assets/video/tissue_paper.mp4" width="100%"
                      style="border-radius:10px; "></video>
                <figcaption>Robot picking up tissue <font color="red"><b>paper</b></font></figcaption>
                </figure>
              </div>
            </div>
            <br>
            <div class="row">
              <div class="col">
                <figure>
                <video autoplay muted loop playsinline controls src="assets/video/tissue_box_human.mp4" width="100%"
                      style="border-radius:10px; "></video>
                <figcaption>Human picking up tissue <font color="purple"><b>box</b></font></figcaption>
                </figure>
              </div>
              <div class="col">
                <figure>
                <video autoplay muted loop playsinline controls src="assets/video/tissue_box.mp4" width="100%"
                      style="border-radius:10px; "></video>
                <figcaption>Robot picking up tissue <font color="purple"><b>box</b></font></figcaption>
                </figure>
              </div>
            </div>
            <br>
            <td>
              <div class="content has-text-justified">
                The following example of <em><b>Tom and Jerry</b></em> shows that our method is able to bridge the great domain gap between the real world and cartoon images, thanks to the generalizability of visual foundation models.
              </div>
            </td>
            <div class="row">
              <div class="col" style="text-align:center">
                <figure>
                <video autoplay muted loop playsinline controls src="assets/video/tom_open.webm" width="72%"
                      style="border-radius:10px; "></video>
                <figcaption>
                  Cat <font color="red"><b>opening</b></font> the drawer<br>
                  (Recommend Chrome for better compatibility)
                </figcaption>
                </figure>
              </div>
              <div class="col">
                <figure>
                <video autoplay muted loop playsinline controls src="assets/video/drawer_open.mp4" width="100%"
                      style="border-radius:10px; "></video>
                <figcaption>Robot <font color="red"><b>opening</b></font> the drawer</figcaption>
                </figure>
              </div>
            </div>
            <br>
            <div class="row">
              <div class="col" style="text-align:center">
                <figure>
                <video autoplay muted loop playsinline controls src="assets/video/tom_close.webm" width="72%"
                      style="border-radius:10px; "></video>
                <figcaption>
                  Cat <font color="purple"><b>closing</b></font> the drawer<br>
                  (Recommend Chrome for better compatibility)
                </figcaption>
                </figure>
              </div>
              <div class="col">
                <figure>
                <video autoplay muted loop playsinline controls src="assets/video/drawer_close.mp4" width="100%"
                      style="border-radius:10px; "></video>
                <figcaption>Robot <font color="purple"><b>closing</b></font> the drawer</figcaption>
                </figure>
              </div>
            </div>
            <br>
          </td>
        </tr>
      </div> 
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen content">
    <h2 class="title is-3">LLM/VLM Integration (3x)</h2>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- <h2 class="title is-2" style="text-align: center; padding-bottom: 10px;"> </h2> -->
        <tr>
          <td>
              <!-- <h3 class="subtitle">
                All rollouts are fully autonomous, <font color="red">WITHOUT</font> any heuristics.
              </h3> -->
              <div class="content has-text-justified">
                Our method can also be easily integrated with LLMs/VLMs for openset instructions and long-horizon tasks, by decomposing them into smaller ones suitable for affordance transfer and other action primitives. Deployment on the Unitree B1Z1 also shows our method's cross-embodiment nature.
              </div>
          </td>
        </tr>
        <tr>
          <td>
            <div class="row">
              <div class="col">
                <figure>
                <video autoplay muted loop playsinline controls src="assets/video/llm.mp4" width="100%"
                      style="border-radius:10px; "></video>
                <figcaption>
                  "Clear the table."
                </figcaption>
                </figure>
              </div>
            </div>
            <br>
          </td>
        </tr>
      </div> 
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>TODO</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://open-world-mobilemanip.github.io/">Open-World Mobile Manipulation</a>, <a href="https://voxposer.github.io/">VoxPoser</a>, <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a href="https://peract.github.io/">PerAct</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
